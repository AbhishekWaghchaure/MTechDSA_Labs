{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb4cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of Reinforcement Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568ec9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15133a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f1999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.ppo.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ce54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4570c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    env = model.get_env()\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d3d8ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 23.52 Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "mean_reward_before_train = evaluate(model, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223b7fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b1be14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:9.55 +/- 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "207bb7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1542baeb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9143202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:441.12 +/- 92.79\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79eae45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: Xvfb: command not found\n"
     ]
    }
   ],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81b40a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_videos(video_path='', prefix=''):\n",
    " \n",
    "  html = []\n",
    "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "      html.append('''<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1151e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
    "  \"\"\"\n",
    "  :param env_id: (str)\n",
    "  :param model: (RL model)\n",
    "  :param video_length: (int)\n",
    "  :param prefix: (str)\n",
    "  :param video_folder: (str)\n",
    "  \"\"\"\n",
    "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "  # Start the video at step=0 and record 500 steps\n",
    "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
    "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
    "                              name_prefix=prefix)\n",
    "\n",
    "  obs = eval_env.reset()\n",
    "  for _ in range(video_length):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "  # Close the video recorder\n",
    "  eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4da84c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py:15\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 15\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyglet\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pyglet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrecord_video\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mCartPole-v1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvideo_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mppo-cartpole\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [14]\u001B[0m, in \u001B[0;36mrecord_video\u001B[0;34m(env_id, model, video_length, prefix, video_folder)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Start the video at step=0 and record 500 steps\u001B[39;00m\n\u001B[1;32m     13\u001B[0m eval_env \u001B[38;5;241m=\u001B[39m VecVideoRecorder(eval_env, video_folder\u001B[38;5;241m=\u001B[39mvideo_folder,\n\u001B[1;32m     14\u001B[0m                             record_video_trigger\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m step: step \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, video_length\u001B[38;5;241m=\u001B[39mvideo_length,\n\u001B[1;32m     15\u001B[0m                             name_prefix\u001B[38;5;241m=\u001B[39mprefix)\n\u001B[0;32m---> 17\u001B[0m obs \u001B[38;5;241m=\u001B[39m \u001B[43meval_env\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(video_length):\n\u001B[1;32m     19\u001B[0m   action, _ \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(obs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:68\u001B[0m, in \u001B[0;36mVecVideoRecorder.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvObs:\n\u001B[1;32m     67\u001B[0m     obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvenv\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m---> 68\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_video_recorder\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obs\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:80\u001B[0m, in \u001B[0;36mVecVideoRecorder.start_video_recorder\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     75\u001B[0m base_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvideo_folder, video_name)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvideo_recorder \u001B[38;5;241m=\u001B[39m video_recorder\u001B[38;5;241m.\u001B[39mVideoRecorder(\n\u001B[1;32m     77\u001B[0m     env\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, base_path\u001B[38;5;241m=\u001B[39mbase_path, metadata\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_id}\n\u001B[1;32m     78\u001B[0m )\n\u001B[0;32m---> 80\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvideo_recorder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcapture_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecorded_frames \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecording \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:132\u001B[0m, in \u001B[0;36mVideoRecorder.capture_frame\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    129\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCapturing video frame: path=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath)\n\u001B[1;32m    131\u001B[0m render_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mansi\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mansi_mode \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 132\u001B[0m frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrender_mode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m frame \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_async:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:87\u001B[0m, in \u001B[0;36mDummyVecEnv.render\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;124;03mGym environment rendering. If there are multiple environments then\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;124;03mthey are tiled together in one image via ``BaseVecEnv.render()``.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03m:param mode: The rendering type.\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 87\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrender(mode\u001B[38;5;241m=\u001B[39mmode)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/core.py:295\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self, mode, **kwargs)\u001B[0m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:179\u001B[0m, in \u001B[0;36mCartPoleEnv.render\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m    176\u001B[0m cartheight \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m30.0\u001B[39m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01menvs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassic_control\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rendering\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewer \u001B[38;5;241m=\u001B[39m rendering\u001B[38;5;241m.\u001B[39mViewer(screen_width, screen_height)\n\u001B[1;32m    182\u001B[0m     l, r, t, b \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mcartwidth \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m, cartwidth \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m, cartheight \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39mcartheight \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyglet\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m    Cannot import pyglet.\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;124;03m    HINT: you can install pyglet directly via 'pip install pyglet'.\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124;03m    But if you really just want to install all Gym dependencies and not have to think about it,\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124;03m    'pip install -e .[all]' or 'pip install gym[all]' will do it.\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     )\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyglet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "\u001B[0;31mImportError\u001B[0m: \n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    "
     ]
    }
   ],
   "source": [
    "record_video('CartPole-v1', model, video_length=500, prefix='ppo-cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_videos('videos', prefix='ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842c6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
